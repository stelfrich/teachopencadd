{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T035 Â· GNN-based molecular property prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This talktorial is a part of TeachOpenCADD, a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects.\n",
    "\n",
    "Authors:\n",
    "\n",
    "* Paula Linh Kramer, 2022, [Volkamer Lab](https://volkamerlab.org/), [NextAID](https://nextaid.cs.uni-saarland.de/) project, Saarland University"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "In this tutorial, we will first explain the basic concepts of graph neural networks (GNNs) and present two different GNN architectures. We apply our neural networks to the `QM9` dataset, which is a dataset containing small molecules. With this dataset, we want to predict molecular properties. We demonstrate how to train and evaluate GNNs step by step using PyTorch Geometric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "* GNN Tasks\n",
    "* Message Passing\n",
    "* Graph Convolutional Network (GCN)\n",
    "* Graph Isomorphism Network (GIN)\n",
    "* Training a GNN\n",
    "* Applications of GNNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "* Dataset\n",
    "* Defining a GCN and GIN\n",
    "* Training a GNN\n",
    "* Evaluating the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* Articles:\n",
    "    * Atz, Kenneth, Francesca Grisoni, and Gisbert Schneider. *Geometric Deep Learning on Molecular Representations*, [Nature Machine Intelligence 3.12 (2021): 1023-1032](https://arxiv.org/pdf/2107.12375.pdf)\n",
    "    * Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. *How Powerful are Graph Neural Networks?*, [International Conference on Learning Representations (ICLR 2019)](https://arxiv.org/abs/1810.00826v3)\n",
    "    * Welling, Max, and Thomas N. Kipf. *Semi-supervised classification with graph convolutional networks*, [International Conference on Learning Representations (ICLR 2017)](https://arxiv.org/pdf/1609.02907.pdf)\n",
    "    * Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. *Neural Message Passing for Quantum Chemistry*, [International conference on machine learning. PMLR, 2017](https://arxiv.org/pdf/1704.01212.pdf)\n",
    "\n",
    "\n",
    "* Blog posts:\n",
    "    * Maxime Labonne, *Graph Convolutional Networks: Introduction to GNNs*, [Maxime Labonne](https://mlabonne.github.io/blog/intrognn/)\n",
    "    * Maxime Labonne, *GIN: How to Design the Most Powerful Graph Neural Network*, [Maxime Labonne](https://mlabonne.github.io/blog/gin/)\n",
    "    * Vortana Say, *How To Save and Load Model In PyTorch With A Complete Example*, [towardsdatascience](https://towardsdatascience.com/how-to-save-and-load-a-model-in-pytorch-with-a-complete-example-c2920e617dee)\n",
    "    * Michael Bronstein, *Expressive power of graph neural networks and the Weisfeiler-Lehman test*, [towardsdatascience](https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49)\n",
    "    * Benjamin Sanchez-Lengeling,  Emily Reif, *A Gentle Introduction to Graph Neural Networks*, [Distill](https://distill.pub/2021/gnn-intro/)\n",
    "\n",
    "\n",
    "* Tutorials:\n",
    "    * *Pytorch Geometric Documentation*, [Colab Notebooks and Video Tutorials](https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html)\n",
    "    * *Pytorch Geometric Documentation*, [Introduction by Example](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#learning-methods-on-graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "### Graph Neural Networks\n",
    "\n",
    "There are several ways to represent molecules which are explained and discussed in **Talktorial T033**. If we work with molecules, one intuitive approach to apply deep learning to certain tasks is to make use of the graph structure of molecules. Graph neural networks can directly work on given graphs. Molecules can easily be represented as a graph, as seen in Figure 1. Given a graph $G=(V, E)$, $V$ describes the vertices or nodes. In molecular graphs, a node $v_i \\in \\mathbb{R}^{d_v}$ represents an atom. Nodes can have $d_v$ different features, such as atomic number and chirality. Edges usually correspond to covalent bonds between the atoms. Each edge $e_{ij} \\in \\mathbb{R}^{d_e}$ is described by $d_e$ number of features, which usually represent the bond type. A graph neural network is a network consisting of learnable and differentiable functions that are invariant for graph permutations. Graph neural networks consist of so-called message-passing layers which will be explained in more detail below, followed by more specific explanations of two different GNN architectures. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/simple-graph.png\" alt=\"simple_graph\" width=\"600\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <em> Figure 1: Molecular graph overview. Figure taken from [<a href=\"https://arxiv.org/pdf/2107.12375.pdf\" target=\"_top\">1</a>]\n",
    "</em>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GNN Tasks\n",
    "\n",
    "We can perform different tasks with a GNN: \n",
    "\n",
    "- Graph-level tasks: one application would be to predict a specific property of the entire graph. This can be a classification task such as toxicity prediction or a regression task. In this tutorial, we will implement a regression task to predict molecular properties. Another graph-level task would be to predict entirely new graphs/molecules. This is especially relevant in the area of drug discovery, where new drug candidates are of interest. \n",
    "- Node-level tasks: we can predict a property of a specific node in the graph, e.g. the atomic charges of each atom. We could also predict a new node to be added to the graph. This is often done for molecule generation, where we want to add multiple atoms to form new molecules one after the other. \n",
    "- Edge-level tasks: we can predict edge properties, e.g. intramolecular forces between atoms, or a new edge in the graph. In the molecule generation context, we want to predict potential bonds between the atoms. Edge prediction can also be used to infer connections/interactions e.g. in a gene regulatory network. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message Passing\n",
    "\n",
    "Instead of MLP layers in standard neural networks, GNNs have message-passing layers, where we collect information about the neighboring nodes. For each node $v$, we look at the direct neighbors $N(v)$ and gather information. Then all the information is aggregated, for example with summation. Then we update the node $v$ with the aggregated messages. If we perform this aggregation and combining, each node contains the information about the direct neighbors (1-hop). If we repeat this $n$ times, we aggregate information about the $n_{th}$ closest neighbors ($n$ -hop). \n",
    "\n",
    "$$a_v^{(k)} = \\text{aggregate}^{(k)} (\\{ h_u^{(k-1)}: u \\in N(v) \\})$$\n",
    "\n",
    "$$h_v^{(k)} = \\text{combine}^{(k)} (h_v^{(k-1)}, a_v^{(k)})$$\n",
    "\n",
    "where $h_v^{(k)}$ is the embedding of node $v$ at layer $k$, $N(v)$ are the neighbors of node $v$. \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/gnn_overview.png\" alt=\"simple_graph\" width=\"600\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <em> Figure 2: Message passing overview. Figure taken from [<a href=\"https://medium.com/stanford-cs224w/self-supervised-learning-for-graphs-963e03b9f809\" target=\"_top\">2</a>]\n",
    "</em>\n",
    "</p>\n",
    "\n",
    "One important property of a GNN is permutation invariance. This means that changing the order of nodes in the graph should not affect the outcome. For example, when working with adjacency matrices, changing the order of nodes would mean swapping rows and/or columns. However, this does not change any properties of a graph, but the input would differ. In GNNs, we want to overcome this. We, therefore need an aggregation function and a combining function that are permutation invariant, such as using the mean, the maximum or a sum. \n",
    "Using a permutation invariant aggregation function ensures that the graph-level outputs are also invariant to permutations. \n",
    "In this tutorial, we will explain graph-level regression tasks and in the following, we will present two different GNN architectures.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN \n",
    "\n",
    "One of the simplest GNNs is a Graph Convolutional Network (GCN). For GCNs, we sum over all neighbors of node $v$, including the node $v$ itself and aggregate all information. We divide it by the degree to keep the range of different nodes comparable. The node-wise aggregation function for layer $k$ is\n",
    "\n",
    "$$h_v^{(k)} = \\Theta^{\\top} \\sum_{u \\in N(v) \\cup \\{v\\}} \\frac{1}{\\sqrt{d_v d_u}} \\cdot h_u^{(k-1)}$$\n",
    "\n",
    "where $d_j$ and $d_i$ denote the degree of node $j$ and $i$, respectively, and $\\Theta$ represent trainable weights. \n",
    "\n",
    "One disadvantage of GCNs is, that they use a mean-based aggregation and this function is not injective. This means that different graphs can lead to the same graph embedding and the network cannot distinguish between the two graphs anymore. One example is visualized in Figure 3 below. Assuming the node and edge properties are identical, GCNs could create the same hidden embedding for these two graphs.  \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/graph.jpeg\" alt=\"simple_graph\" width=\"500\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <em> Figure 3: Two indistinguishable graphs using GCNs \n",
    "</em>\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### GIN \n",
    "Another type of GNN is the Graph Isomorphism Network (GIN), which has been proposed to overcome the disadvantages of GCNs explained above. The aggregation function is defined as follows\n",
    "\n",
    "$$h_v^{(k)} = h_\\Theta((1+ \\epsilon) \\cdot h_v^{(k-1)} + \\sum_{u \\in N(v)} h_u^{(k-1)} )$$\n",
    "\n",
    "The aggregation function here is a sum. The parameter $\\epsilon$ decides on the importance of the node $v$ compared to its neighbors. $h_\\Theta$ represents a neural network for all nodes $v$, for example an MLP. The sum aggregation function is more powerful compared to a mean aggregation (used in the GCN above) since we can distinguish between more similar graphs, for example, the two graphs in Figure 3.  \n",
    "\n",
    "\n",
    "GINs are a good example of a simple network, which still is quite powerful, as they are quite good at distinguishing between non-isomorphic graphs. Two graphs are isomorphic if the graphs are identical except for node permutations. While this might be easily visible for smaller graphs, it is a complex problem for larger graphs. When working with GNNs, we would like the model to give us the same output if the input graphs are isomorphic. On the other hand, we also want the model to be able to differentiate between non-isomorphic graphs and output (possibly) different results. GINs can differentiate between non-isomorphic graphs a lot better than other simple GNNs such as GCN and GraphSage. For example, the two graphs in the figure above have different embeddings using GINs, since we are using a sum-based aggregation without any scaling or averaging. It is proven that GINs are as powerful as the Weisfeiler-Lehman test, a common (but not perfect) isomorphism test for graphs. If you are interested in the WL test or more details on GINs, have a look at the original publication about [GINs](https://arxiv.org/abs/1810.00826v3) or this [blog post](https://towardsdatascience.com/expressive-power-of-graph-neural-networks-and-the-weisefeiler-lehman-test-b883db3c7c49) about the WL test. GINs cannot distinguish between all non-isomorphic graphs, one example is in Figure 4. Each node in both graphs has the same number of neighbors, therefore $h_v$ is the same for all nodes $v$ in both graphs. \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/gin_graphs.jpeg\" alt=\"simple_graph\" width=\"500\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <em> Figure 4: Two indistinguishable graphs using GINs\n",
    "</em>\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a GNN\n",
    "\n",
    "Similar to training a standard neural network, different design choices and hyperparameters need to be decided on. We will shortly present some concepts commonly used in neural networks, which can also be used for GNNs. Loss functions and activation functions are already discussed in **Talktorial T022**. We also used the mean squared error loss as well as the ReLU activation function. \n",
    "\n",
    "\n",
    "##### Batching\n",
    "\n",
    "It is common to do batching when training a GNN to improve performance. The batch size indicates how many samples from the training data are fed to the neural network before updating model parameters. Choosing the right batch size is a trade-off between computational cost and generalization. For larger batches, the model is updated fewer times and the training is a lot faster. Models using smaller batches can generalize better, meaning that the test error can be lowered. Since this is not the only hyperparameter, choosing the batch size is also linked to the learning rate, the number of training epochs etc. One way to implement batching in GNNs is to stack the adjacency matrices of all graphs in the batch diagonally and to concatenate the node feature matrices. However, graphs (especially molecular graphs) can have rather sparse adjacency matrices. In this case, it is more efficient to use a sparse representation for the edges. PyTorch Geometric for example uses [edge lists](https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html), where only the indexes of present edges are saved. These lists are concatenated during batching. \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/batching-ex.png\" alt=\"simple_graph\" width=\"600\"/>\n",
    "</p>\n",
    "<p align=\"center\">\n",
    "    <em> Figure 4: Batching in GNNs, image taken from [<a href=\"https://blog.dataiku.com/graph-neural-networks-part-three\" target=\"_top\">3</a>]\n",
    "</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "#### Pooling\n",
    "\n",
    "Pooling layers help a neural network to reduce dimensionality. This makes the model more robust to variations. In graphs, global pooling layers can produce a graph embedding from the different node embeddings. There are different ways for pooling, the most common ones are: mean, max and sum, which are permutation invariant. Hence, pooling layers are also permutation invariant. \n",
    "For our GCN, we use a global mean pooling layer and for our GIN we use a global sum pooling layer, as it was proposed in the original publications listed in the references above. Pooling layers are also very useful to reduce the size of the layer to a fixed size for graph representation, therefore global pooling layers are also referred to as readout layers. \n",
    "\n",
    "\n",
    "\n",
    "#### Dropout (Regularization)\n",
    "\n",
    "One common problem in deep learning tasks is overfitting. This usually means that the dataset used to train the neural network is too small. Applying an overfitted network to a different dataset then leads to a high error in prediction, since the model is fit too closely to the training data and does not generalize well enough. To reduce overfitting, one approach is to use dropout layers, which can lead to a better generalization of the model. During training, nodes are randomly dropped. The probability of dropping nodes is another hyperparameter to be fixed. In each iteration, the nodes in a neural network (and the number of nodes) can therefore differ. This means we incorporate more noise and therefore force the neural network to generalize better. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of GNNs\n",
    "\n",
    "GNNs can be applied to a wide variety of tasks involving graphs, these could be based on small molecules (like in this tutorial), but also proteins (see **Talktorial T038**), gene regulatory networks and many more. Some applications are: \n",
    "\n",
    "* Property prediction of molecules, such as toxicity and solubility (see: Wieder, Oliver, et al. *A compact review of molecular property prediction with graph neural networks* [Drug Discovery Today: Technologies 37 (2020): 1-12.](https://www.sciencedirect.com/science/article/pii/S1740674920300305) and *MoleculeNet: a benchmark for molecular machine learning* by Zhenqin Wu et al., [Chemical science 9.2 (2018): 513-530.](https://pubs.rsc.org/en/content/articlehtml/2018/sc/c7sc02664a))\n",
    "* Generating new molecules, which is especially relevant in the field of drug discovery (for more details, read this review by Tong, Xiaochu, et al. *Generative models for De Novo drug design* [Journal of Medicinal Chemistry 64.19 (2021): 14011-14027](https://pubs.acs.org/doi/full/10.1021/acs.jmedchem.1c00927?casa_token=WhlMtHT6bdEAAAAA%3ATT5MISL_F3LN9lEnddHjZsNpQwuCycQgN02rIYfuSL2BSki12AdH72H4i2KwlhaIltWUPC0ia1g61YQ))\n",
    "* Inferring new interactions/associations in biological networks, such as gene regulatory networks or protein-protein interaction networks\n",
    "\n",
    "For a more detailed overview of GNNs and their applications, you can read the article by Zhang, Xiao-Meng, et al. *Graph Neural Networks and Their Current Applications in Bioinformatics* [Frontiers in Genetics 12 (2021)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8360394/). \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical \n",
    "\n",
    "For the practical section, we have used PyTorch and PyTorch-Geometric, which helps us to handle graph data efficiently. PyTorch Geometric for example uses sparse matrix representations and implemented efficient graph batching. However, there are also different graph libraries for Python, such as the [Deep Graph Library](https://www.dgl.ai/) which is not covered in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:11:09.797897100Z",
     "start_time": "2024-01-18T11:11:03.399897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joere\\miniconda3\\envs\\T034\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as Fun\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:11:09.814606700Z",
     "start_time": "2024-01-18T11:11:09.775649900Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if sys.platform.startswith((\"linux\", \"darwin\")):\n",
    "    !mamba install -q -y -c pyg pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:11:16.266219700Z",
     "start_time": "2024-01-18T11:11:09.786147Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.nn import GCNConv, GINConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:11:16.405029900Z",
     "start_time": "2024-01-18T11:11:16.270823200Z"
    }
   },
   "outputs": [],
   "source": [
    "# specify the local data path\n",
    "HERE = Path(_dh[-1])\n",
    "DATA = HERE / \"data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "For this tutorial, we use the [QM9](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.QM9) dataset, which can be imported with `torch_geometric`. The dataset is part of a benchmarking collection called [MoleculeNet](https://pubs.rsc.org/en/content/articlehtml/2018/sc/c7sc02664a). It contains around $130,000$ small molecules with at most 9 heavy atoms as well as various molecular properties. We will choose one property which we will then try to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:01.587310300Z",
     "start_time": "2024-01-18T11:11:16.405029900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
      "Extracting C:\\Users\\joere\\Desktop\\Dokumente\\Workspace\\Python\\teachopencadd\\teachopencadd\\talktorials\\T035_graph_neural_networks\\data\\raw\\qm9_v3.zip\n",
      "Processing...\n",
      "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "qm9 = QM9(root=DATA)\n",
    "qm9[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this tutorial for the first time, the dataset will be downloaded here. As an example, the first molecule from the dataset is shown below. The dataset contains the following information:\n",
    "- `x`: contains the different node features, such as atomic number, chirality, hybridization, is aromatic, is ring,\n",
    "- `edge_index`: adjacency matrix, representing the covalent bonds between the atoms,\n",
    "- `edge_attributes`: contains the edge features (bond type, is conjugated, stereo configuration),\n",
    "- `pos`: 3D atom coordinates, we will not use them in this tutorial,\n",
    "- `z`: atomic numbers,\n",
    "- `y`: target values, this dataset contains 19 different properties describing each molecule, such as dipole moment, different molecular energies, enthalpy and rotational constants. \n",
    "\n",
    "\n",
    "In this tutorial, we only use `x`, `edge_index` and `y` to keep it simple. While the dataset has many regression targets, we will only focus on one of the tasks, which is the prediction of the dipole moment $\\mu$. For this tutorial, we only sample a subset of QM9. This keeps the runtime low and this is still enough to show some first results. The dataset is split into training, validation and test sets with a $80:10:10$ split ratio. In addition, we normalize the training data ($\\mu=0, \\sigma=1$) and apply the same mean and standard deviation to the test and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:01.733739700Z",
     "start_time": "2024-01-18T11:12:01.591690600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joere\\miniconda3\\envs\\T034\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The data of the dataset is already cached, so any modifications to `data` will not be reflected when accessing its elements. Clearing the cache now by removing all elements in `dataset._data_list`. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\joere\\miniconda3\\envs\\T034\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\joere\\miniconda3\\envs\\T034\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:284: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. The given 'InMemoryDataset' only references a subset of examples of the full dataset, but 'data' will contain information of the full dataset. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# get one regression target\n",
    "y_target = pd.DataFrame(qm9.data.y.numpy())\n",
    "qm9.data.y = torch.Tensor(y_target[0])\n",
    "\n",
    "qm9 = qm9.shuffle()\n",
    "\n",
    "# data split\n",
    "data_size = 30000\n",
    "train_index = int(data_size * 0.8)\n",
    "test_index = train_index + int(data_size * 0.1)\n",
    "val_index = test_index + int(data_size * 0.1)\n",
    "\n",
    "\n",
    "# normalizing the data\n",
    "data_mean = qm9.data.y[0:train_index].mean()\n",
    "data_std = qm9.data.y[0:train_index].std()\n",
    "\n",
    "qm9.data.y = (qm9.data.y - data_mean) / data_std\n",
    "\n",
    "# datasets into DataLoader\n",
    "train_loader = DataLoader(qm9[0:train_index], batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(qm9[train_index:test_index], batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(qm9[test_index:val_index], batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a GCN and a GIN\n",
    "\n",
    "The following two Python classes are the two GNNs we will consider in this tutorial. Both have 3 convolutional layers, one global pooling layer, linear layers, ReLU activation functions between the layers and a dropout layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:01.772390800Z",
     "start_time": "2024-01-18T11:12:01.743844300Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    \"\"\"Graph Convolutional Network class with 3 convolutional layers and a linear layer\"\"\"\n",
    "\n",
    "    def __init__(self, dim_h):\n",
    "        \"\"\"init method for GCN\n",
    "\n",
    "        Args:\n",
    "            dim_h (int): the dimension of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(qm9.num_features, dim_h)\n",
    "        self.conv2 = GCNConv(dim_h, dim_h)\n",
    "        self.conv3 = GCNConv(dim_h, dim_h)\n",
    "        self.lin = torch.nn.Linear(dim_h, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        e = data.edge_index\n",
    "        x = data.x\n",
    "\n",
    "        x = self.conv1(x, e)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, e)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, e)\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        x = Fun.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:01.810363300Z",
     "start_time": "2024-01-18T11:12:01.770192500Z"
    }
   },
   "outputs": [],
   "source": [
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"Graph Isomorphism Network class with 3 GINConv layers and 2 linear layers\"\"\"\n",
    "\n",
    "    def __init__(self, dim_h):\n",
    "        \"\"\"Initializing GIN class\n",
    "\n",
    "        Args:\n",
    "            dim_h (int): the dimension of hidden layers\n",
    "        \"\"\"\n",
    "        super(GIN, self).__init__()\n",
    "        self.conv1 = GINConv(\n",
    "            Sequential(Linear(11, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU())\n",
    "        )\n",
    "        self.conv2 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.conv3 = GINConv(\n",
    "            Sequential(\n",
    "                Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(), Linear(dim_h, dim_h), ReLU()\n",
    "            )\n",
    "        )\n",
    "        self.lin1 = Linear(dim_h, dim_h)\n",
    "        self.lin2 = Linear(dim_h, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        # Node embeddings\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.relu()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.relu()\n",
    "        h = self.conv3(h, edge_index)\n",
    "\n",
    "        # Graph-level readout\n",
    "        h = global_add_pool(h, batch)\n",
    "\n",
    "        h = self.lin1(h)\n",
    "        h = h.relu()\n",
    "        h = Fun.dropout(h, p=0.5, training=self.training)\n",
    "        h = self.lin2(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a GNN\n",
    "\n",
    "When training a GNN (or any neural network), we have a training set, a validation set and a test set. The training set is used for training, the validation set is used to test the loss in each epoch not only on the training set but also on another dataset (*monitor generalization performance*). The test set is used to calculate the error of the fully trained model using a dataset, which has not been used during the whole training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:01.841355300Z",
     "start_time": "2024-01-18T11:12:01.812581200Z"
    }
   },
   "outputs": [],
   "source": [
    "def training(loader, model, loss, optimizer):\n",
    "    \"\"\"Training one epoch\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): loader (DataLoader): training data divided into batches\n",
    "        model (nn.Module): GNN model to train on\n",
    "        loss (nn.functional): loss function to use during training\n",
    "        optimizer (torch.optim): optimizer during training\n",
    "\n",
    "    Returns:\n",
    "        float: training loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0\n",
    "    for d in loader:\n",
    "        optimizer.zero_grad()\n",
    "        d.x = d.x.float()\n",
    "\n",
    "        out = model(d)\n",
    "\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
    "        current_loss += l / len(loader)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    return current_loss, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:01.875807200Z",
     "start_time": "2024-01-18T11:12:01.849741400Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(loader, model, loss):\n",
    "    \"\"\"Validation\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): validation set in batches\n",
    "        model (nn.Module): current trained model\n",
    "        loss (nn.functional): loss function\n",
    "\n",
    "    Returns:\n",
    "        float: validation loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for d in loader:\n",
    "        out = model(d)\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
    "        val_loss += l / len(loader)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:01.909688200Z",
     "start_time": "2024-01-18T11:12:01.878919300Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def testing(loader, model):\n",
    "    \"\"\"Testing\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): test dataset\n",
    "        model (nn.Module): trained model\n",
    "\n",
    "    Returns:\n",
    "        float: test loss\n",
    "    \"\"\"\n",
    "    loss = torch.nn.MSELoss()\n",
    "    test_loss = 0\n",
    "    test_target = numpy.empty((0))\n",
    "    test_y_target = numpy.empty((0))\n",
    "    for d in loader:\n",
    "        out = model(d)\n",
    "        # NOTE\n",
    "        # out = out.view(d.y.size())\n",
    "        l = loss(out, torch.reshape(d.y, (len(d.y), 1)))\n",
    "        test_loss += l / len(loader)\n",
    "\n",
    "        # save prediction vs ground truth values for plotting\n",
    "        test_target = numpy.concatenate((test_target, out.detach().numpy()[:, 0]))\n",
    "        test_y_target = numpy.concatenate((test_y_target, d.y.detach().numpy()))\n",
    "\n",
    "    return test_loss, test_target, test_y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:01.940282300Z",
     "start_time": "2024-01-18T11:12:01.914788500Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epochs(epochs, model, train_loader, val_loader, path):\n",
    "    \"\"\"Training over all epochs\n",
    "\n",
    "    Args:\n",
    "        epochs (int): number of epochs to train for\n",
    "        model (nn.Module): the current model\n",
    "        train_loader (DataLoader): training data in batches\n",
    "        val_loader (DataLoader): validation data in batches\n",
    "        path (string): path to save the best model\n",
    "\n",
    "    Returns:\n",
    "        array: returning train and validation losses over all epochs, prediction and ground truth values for training data in the last epoch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    loss = torch.nn.MSELoss()\n",
    "\n",
    "    train_target = numpy.empty((0))\n",
    "    train_y_target = numpy.empty((0))\n",
    "    train_loss = numpy.empty(epochs)\n",
    "    val_loss = numpy.empty(epochs)\n",
    "    best_loss = math.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, model = training(train_loader, model, loss, optimizer)\n",
    "        v_loss = validation(val_loader, model, loss)\n",
    "        if v_loss < best_loss:\n",
    "            torch.save(model.state_dict(), path)\n",
    "        for d in train_loader:\n",
    "            out = model(d)\n",
    "            if epoch == epochs - 1:\n",
    "                # record truly vs predicted values for training data from last epoch\n",
    "                train_target = numpy.concatenate((train_target, out.detach().numpy()[:, 0]))\n",
    "                train_y_target = numpy.concatenate((train_y_target, d.y.detach().numpy()))\n",
    "\n",
    "        train_loss[epoch] = epoch_loss.detach().numpy()\n",
    "        val_loss[epoch] = v_loss.detach().numpy()\n",
    "\n",
    "        # print current train and val loss\n",
    "        if epoch % 2 == 0:\n",
    "            print(\n",
    "                \"Epoch: \"\n",
    "                + str(epoch)\n",
    "                + \", Train loss: \"\n",
    "                + str(epoch_loss.item())\n",
    "                + \", Val loss: \"\n",
    "                + str(v_loss.item())\n",
    "            )\n",
    "    return train_loss, val_loss, train_target, train_y_target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We have trained both models with 100 epochs and saved the best models under `GCN_best-model-parameters.pt` and `GIN_best-model-parameters.pt`. Since this takes some time, we reduced the number of epochs to 10 for this tutorial for demonstration purposes. The results and the plots below are based on the models trained for 100 epochs. If you want to train your own model using our tutorial, you can change the number of epochs and any other parameters in our models (such as learning rate, batch size, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:12:39.967171500Z",
     "start_time": "2024-01-18T11:12:01.943766600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training GCN for 10 epochs\n",
    "epochs = 10\n",
    "\n",
    "model = GCN(dim_h=128)\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "gcn_train_loss, gcn_val_loss, gcn_train_target, gcn_train_y_target = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"GCN_model.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:12:39.857216900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training GIN for 10 epochs\n",
    "model = GIN(dim_h=64)\n",
    "\n",
    "# Remember to change the path if you want to keep the previously trained model\n",
    "gin_train_loss, gin_val_loss, gin_train_target, gin_train_y_target = train_epochs(\n",
    "    epochs, model, train_loader, test_loader, \"GIN_model.pt\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model \n",
    "\n",
    "For evaluation, we use a validation dataset to find the best model and a test set, to test our model on unseen data. \n",
    "First, we plotted the losses of our training and validation sets. As expected, the GIN model has a lower training and validation loss. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:12:39.874475800Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(gcn_train_loss, gcn_val_loss, gin_train_loss, gin_val_loss):\n",
    "    \"\"\"Plot the loss for each epoch\n",
    "\n",
    "    Args:\n",
    "        epochs (int): number of epochs\n",
    "        train_loss (array): training losses for each epoch\n",
    "        val_loss (array): validation losses for each epoch\n",
    "    \"\"\"\n",
    "    plt.plot(gcn_train_loss, label=\"Train loss (GCN)\")\n",
    "    plt.plot(gcn_val_loss, label=\"Val loss (GCN)\")\n",
    "    plt.plot(gin_train_loss, label=\"Train loss (GIN)\")\n",
    "    plt.plot(gin_val_loss, label=\"Val loss (GIN)\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:12:39.876498700Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_targets(pred, ground_truth):\n",
    "    \"\"\"Plot true vs predicted value in a scatter plot\n",
    "\n",
    "    Args:\n",
    "        pred (array): predicted values\n",
    "        ground_truth (array): ground truth values\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.scatter(pred, ground_truth, s=0.5)\n",
    "    plt.xlim(-2, 7)\n",
    "    plt.ylim(-2, 7)\n",
    "    ax.axline((1, 1), slope=1)\n",
    "    plt.xlabel(\"Predicted Value\")\n",
    "    plt.ylabel(\"Ground truth\")\n",
    "    plt.title(\"Ground truth vs prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the losses for each epoch, we can see that the GIN model performs better overall. We can also see that the training loss is often lower compared to the validation loss. This is normal since the training loss describes the error of the model using the training set, which is the dataset used for improving the model. The validation loss is calculated on a separate dataset, which is not used for updating the model weights. Therefore, the error is often higher. This is also the reason, the validation loss sometimes fluctuates more. As long as both losses show a decreasing tendency, this is not problematic. It is important to have a low training loss and a low validation loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:12:39.876498700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot overall losses of GIN and GCN\n",
    "\n",
    "plot_loss(gcn_train_loss, gcn_val_loss, gin_train_loss, gin_val_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we also plotted the actual predictions of our target value compared to the ground truth for the GIN model, since this model performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:12:39.882495500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot target and prediction for training data\n",
    "\n",
    "plot_targets(gin_train_target, gin_train_y_target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have calculated the test loss for both the GCN and the GIN. We also plot the predicted dipole moment compared to the ground truth for both models. If we are interested in the actual numeric range of the predicted dipole moment, the normalization applied during the preprocessing should be subtracted again. Since we only visualize the data in our evaluation, this does not make a difference. In the figures below, we can see that the GIN model performs a lot better compared to the GCN since the test error is lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:12:39.888555800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate test loss from the best GCN model (according to validation loss)\n",
    "\n",
    "# load our model\n",
    "model = GCN(dim_h=128)\n",
    "model.load_state_dict(torch.load(\"GCN_best-model-parameters.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "gcn_test_loss, gcn_test_target, gcn_test_y = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for GCN: \" + str(gcn_test_loss.item()))\n",
    "\n",
    "# plot prediction vs ground truth\n",
    "plot_targets(gcn_test_target, gcn_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:12:39.892554300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate test loss from the best GIN model (according to validation loss)\n",
    "\n",
    "# load our model\n",
    "model = GIN(dim_h=64)\n",
    "model.load_state_dict(torch.load(\"GIN_best-model-parameters.pt\"))\n",
    "\n",
    "# calculate test loss\n",
    "gin_test_loss, gin_test_target, gin_test_y = testing(test_loader, model)\n",
    "\n",
    "print(\"Test Loss for GIN: \" + str(gin_test_loss.item()))\n",
    "\n",
    "# plot prediction vs ground truth\n",
    "plot_targets(gin_test_target, gin_test_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion \n",
    "\n",
    "In this talktorial we have first presented two different graph neural networks. We applied these two GNNs to a molecular dataset to predict molecular properties. We showed how to train and evaluate a simple GNN using *pytorch* and *pytorch_geometric*. This model can be used for any type of graph-level regression and, with small changes (such as the loss function), graph-level classification is also easy. \n",
    "\n",
    "\n",
    "One disadvantage of GNNs is that the quality of the model is extremely data-dependent, the more of the chemical space is covered in the training set, the better the performance would be on new, unseen data. In addition, training a model can be rather complex, since there are many parameters influencing the model. Model parameters, such as learning rate, batch size and number of hidden dimensions could be more thoroughly evaluated to improve the model. To apply this to real tasks, first, a bigger dataset is needed. When using the whole QM9 dataset and not only a small subset, the performance will increase. In addition, the model parameters can also still be optimized. The model architecture can also still be adapted. These changes could lead to longer runtimes, which is why we have chosen this simplified version for demonstration purposes. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "1. What is the difference between a GCN and GIN? \n",
    "2. How would you change the model for a classification task? \n",
    "3. What other parameters can be tuned for better model performance? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "303c4499c6089c66bb06f8fad745a91d2a66aad0a931b516c61543a7644eeb4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
