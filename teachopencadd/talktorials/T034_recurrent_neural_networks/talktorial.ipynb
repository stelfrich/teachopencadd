{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T034 · RNN-based molecular property prediction\n",
    "\n",
    "**Note:** This talktorial is a part of TeachOpenCADD, a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects.\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Azat Tagirdzhanov, 2022, [Chair for Clinical Bioinformatics](https://www.ccb.uni-saarland.de/), [NextAID](https://nextaid.cs.uni-saarland.de/) project, Saarland University"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "Molecular representation by a SMILES string paved the way for applying natural language processing techniques to a broad range of molecule-related tasks. In this talktorial we will dive deeper into one of these techniques: recurrent neural networks (RNNs). First, we will describe different RNN architectures and then apply them to a regression task using the QM9 dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "* Molecules as text\n",
    "    * Tokenization and one-hot encoding\n",
    "* Recurrent Neural Networks (RNNs)\n",
    "    * Vanilla RNN\n",
    "    * Training an RNN\n",
    "    * Vanishing gradients\n",
    "    * Gated Recurrent Unit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "\n",
    "* Dataset\n",
    "* Model definition\n",
    "* Training\n",
    "* Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "#### Talktorials\n",
    "* __Talktorial T021__: One-Hot Encoding\n",
    "* __Talktorial T022__: Ligand-based screening: neural networks\n",
    "* __Talktorial T033__: Molecular Representations\n",
    "* __Talktorial T034__: GNN based property prediction\n",
    "\n",
    "\n",
    "#### Theoretical background\n",
    "* Michael Phi, <i>Illustrated Guide to Recurrent Neural Networks</i>, [towardsdatascience](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
    "* Michael Phi, <i>Illustrated Guide to LSTM’s and GRU’s: A step by-step explanation</i>, [towardsdatascience](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "* [Modern Recurrent Neural Networks](https://d2l.ai/chapter_recurrent-modern/index.html), <i>D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions</i>\n",
    "* Denny Britz, <i>Recurrent Neural Networks Tutorial</i>, [dennybritz.com](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-1/)\n",
    "* Andrej Karpathy, <i>The Unreasonable Effectiveness of Recurrent Neural Networks</i>, [Andrej Karpathy blog](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecules as text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply machine learning to molecular data, it is necessary to first convert the molecules into a representation that can be used as input to the machine learning models. We have already discussed numerous ways of molecular representations in __Talktorial T033__. In this talktorial we will be using a textual representation of molecules by SMILES strings.\n",
    "\n",
    "The string representation of molecules paves the way for applying natural language processing (NLP) techniques to molecular data. In recent years, there has been significant progress in NLP models including, most famously, recurrent neural networks and transformers. These models proved to be good at capturing text semantics and, when applied to SMILES strings, can capture the structure of the molecule in its textual representation.\n",
    "\n",
    "#### Tokenization and one-hot encoding\n",
    "Like other machine learning models, NLP models are designed to operate with numeric inputs. We have already discussed how to transform SMILES strings into numerical form in __Talktorial T021__. Here, we will only briefly go through the key steps:\n",
    "- To transform a string into a sequence of vectors, the string is first split into meaningful chunks called *tokens*. For example, one way of tokenizing a SMILES string `C=CCl` is to split it into individual atomic and branch symbols: `[C, =, C, Cl]`. \n",
    "- All possible tokens form a *vocabulary*. The vocabulary is usually derived from the dataset and contains a fixed amount of the most common tokens among all the data. Depending on the application, the vocabulary might also contain special tokens marking out-of-vocabulary tokens, padding symbols, and more. In our example, only limited to this particular compound, the vocabulary will look like `[C, Cl, =]`. Replacing the tokens with their indices in the vocabulary gets us to another representation, `[0, 2, 0, 1]`. \n",
    "- Finally, the sequence of token indices is transformed to a sequence of binary vectors using one-hot encoding:\n",
    "$$\n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},~\n",
    "\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix},~\n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},~\n",
    "\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks (RNNs) are designed to handle sequential data. To do so, they introduce the notion of time. Elements of the input sequence are processed one after another, with the output of the current time step being passed to the next one as an additional input. This is done by introducing a special type of connection called a *recurrent connection* (hence the name). This architecture allows RNNs to accumulate information about the past and as a result capture dependencies between elements in the sequence.\n",
    "\n",
    "The figure below illustrates the basic principle of an RNN. An RNN cell has a hidden state vector $h_t$ which is updated at each time step $t$ and is responsible for aggregating the information about the previous elements of the input sequence. The update is performed by combining the input $x_t$ with the hidden state of the previous step $h_{t-1}$, passed to the RNN cell by the recurrent connection (Figure 1, on the left). \n",
    "\n",
    "To better visualize how RNN processes sequences of data it is helpful to unfold the network in time (Figure 1, on the right). The RNN is unfolded by duplicating each unit at each time step and sharing the weights and biases across all the units. The unfolded representation shows that RNNs can be viewed as feed-forward neural networks with shared parameters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](images/rnn-unfolded-800.png)\n",
    "\n",
    "*Figure 1:* \n",
    "Two ways of representing a recurrent neural network: compressed (left) and unfolded in time (right). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mechanism behind the computation of the hidden state varies depending on the architecture. Examples of RNN architectures include Vanilla RNN, Long Short-Term Memory (LSTM), and Gated Recurrent Units (GRUs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla RNN\n",
    "\n",
    "In a Vanilla RNN the hidden state is updated according to the following recurrence relation,\n",
    "\n",
    "$$h_{t} = \\tanh( W x_{t} + U h_{t-1} + b),~~~t=1,2,\\dots,L.$$\n",
    "\n",
    "Here $W$ and $U$ are weight matrices, and $b$ is a bias vector. The initial hidden state $h_0$ is usually seeded as a vector of zeros. Model weights $W$, $U$ and $b$ are shared between all the time steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an RNN\n",
    "Similar to other neural networks, RNNs are trained using a backpropagation algorithm. A flavor of the algorithm used in RNNs is called *backpropagation through time* or BPTT for short. BPTT applies backpropagation to the unfolded network to compute the gradient of the loss function for the network weights taking into account parameter sharing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing gradients\n",
    "\n",
    "Vanilla RNNs are susceptible to a problem called *vanishing gradients*. This problem occurs in deep neural networks when the gradients of the weights of the first layers of the network become very small as they are backpropagated through the network. This can lead to slow training and poor performance. In RNNs, the backpropagation is applied to the unfolded network which tends to be very deep. Furthermore, all copies of the RNN cells in the unfolded network share the same weights, which can cause gradients to diminish exponentially fast. For more details on the vanishing gradients problem, please refer to the blog post: [Backpropagation Through Time and Vanishing Gradients](https://dennybritz.com/posts/wildml/recurrent-neural-networks-tutorial-part-3/). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gated Recurrent Unit\n",
    "A Gated recurrent unit (GRU) is a more advanced architecture of the recurrent network that aims to solve the vanishing gradient problem. In addition to the hidden state vector, GRU introduces the so-called *update gate* and *reset gate*. These two vectors help the model to decide which information should be passed forward. They can be trained to keep information from long ago, without washing it through time or removing information that is irrelevant to the prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Reset gate\n",
    "\n",
    "This gate is used by the GRU cell to decide how much of the past information to forget. It is calculated using the formula\n",
    "\n",
    "$$r_t = \\sigma(W_{r} x_t + U_{r} h_{t-1} + b_r).$$\n",
    "\n",
    "The input vector $x_t$ is multiplied by the weight matrix $W_r$. The hidden state from the previous time steps $h_{t-1}$ is also plugged into the cell and is multiplied by its weight matrix $U_r$. Both results are added together with a bias vector $b_r$ and passed through a sigmoid activation function to limit the output to a range between 0 and 1.\n",
    "\n",
    "##### Update gate\n",
    "\n",
    "The update gate is calculated using a similar formula,\n",
    "\n",
    "$$z_t = \\sigma(W_{z} x_t + U_{z} h_{t-1} + b_z),$$\n",
    "\n",
    "but it has its own weights $W_z$, $U_z$, $b_z$, and serves a different purpose. As we will see below, the update gate helps the GRU cell to decide how much of the past information should be passed to the future step.\n",
    "\n",
    "##### Hidden state candidate\n",
    "\n",
    "A candidate's hidden state stores the relevant information from the past. It is calculated as follows:\n",
    "\n",
    "$$\\hat{h}_t = \\tanh(W_{h} x_t + U_{h}(r_t \\odot h_{t-1}) + b_h),$$\n",
    "\n",
    "where $\\odot$ is the [element-wise product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) and $r_t$ is the reset gate vector introduced above. This formula looks similar to how the hidden state is updated in a simple RNN cell, but here the hidden state from the previous time step $h_{t-1}$ is multiplied by the reset gate vector $r_t$. Due to sigmoid activation, components of $r_t$ take values between 0 and 1. Using the reset gate the model can learn which information is not relevant and should be discarded. If, for example, an $i$-th component of $r_t$ is close to 0, then the memory contained in the $i$-th component of $h_{t-1}$ will be discarded in the candidate vector. \n",
    "\n",
    "##### Hidden state update\n",
    "\n",
    "Finally, the hidden state is updated as a mixture of $h_{t-1}$ and the hidden state candidate,\n",
    "\n",
    "$$h_t =   z_t \\odot h_{t-1} + (1-z_t) \\odot  \\hat{h}_t. $$\n",
    "\n",
    "Here, the update gate vector $z_t$ controls how much information to collect from the candidate vector $\\hat{h}_t$ and how much from the previous steps $h_{t-1}$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the practical section, we will apply recurrent neural networks to a regression task.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:24.509108900Z",
     "start_time": "2024-01-18T11:10:24.456159100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:24.749065700Z",
     "start_time": "2024-01-18T11:10:24.504763700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use cuda if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# seed random generator\n",
    "_ = torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:24.794046400Z",
     "start_time": "2024-01-18T11:10:24.756381Z"
    }
   },
   "outputs": [],
   "source": [
    "HERE = Path(_dh[-1])\n",
    "DATA = HERE / \"data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this talktorial we use [QM9](https://pytorch-geometric.readthedocs.io/en/2.2.0/modules/datasets.html#torch_geometric.datasets.QM9) dataset from the [MoleculeNet](https://arxiv.org/abs/1703.00564) paper. This dataset consists of about 130,000 molecules together with their quantum chemical properties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:25.672602600Z",
     "start_time": "2024-01-18T11:10:24.789106100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  mol_id          A           B           C      mu  alpha    homo    lumo  \\\n0  gdb_1  157.71180  157.709970  157.706990  0.0000  13.21 -0.3877  0.1171   \n1  gdb_2  293.60975  293.541110  191.393970  1.6256   9.46 -0.2570  0.0829   \n2  gdb_3  799.58812  437.903860  282.945450  1.8511   6.31 -0.2928  0.0687   \n3  gdb_4    0.00000   35.610036   35.610036  0.0000  16.28 -0.2845  0.0506   \n4  gdb_5    0.00000   44.593883   44.593883  2.8937  12.99 -0.3604  0.0191   \n\n      gap       r2  ...         u0       u298       h298       g298     cv  \\\n0  0.5048  35.3641  ... -40.478930 -40.476062 -40.475117 -40.498597  6.469   \n1  0.3399  26.1563  ... -56.525887 -56.523026 -56.522082 -56.544961  6.316   \n2  0.3615  19.0002  ... -76.404702 -76.401867 -76.400922 -76.422349  6.002   \n3  0.3351  59.5248  ... -77.308427 -77.305527 -77.304583 -77.327429  8.574   \n4  0.3796  48.7476  ... -93.411888 -93.409370 -93.408425 -93.431246  6.278   \n\n      u0_atom   u298_atom   h298_atom   g298_atom  smiles  \n0 -395.999595 -398.643290 -401.014647 -372.471772       C  \n1 -276.861363 -278.620271 -280.399259 -259.338802       N  \n2 -213.087624 -213.974294 -215.159658 -201.407171       O  \n3 -385.501997 -387.237686 -389.016047 -365.800724     C#C  \n4 -301.820534 -302.906752 -304.091489 -288.720028     C#N  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mol_id</th>\n      <th>A</th>\n      <th>B</th>\n      <th>C</th>\n      <th>mu</th>\n      <th>alpha</th>\n      <th>homo</th>\n      <th>lumo</th>\n      <th>gap</th>\n      <th>r2</th>\n      <th>...</th>\n      <th>u0</th>\n      <th>u298</th>\n      <th>h298</th>\n      <th>g298</th>\n      <th>cv</th>\n      <th>u0_atom</th>\n      <th>u298_atom</th>\n      <th>h298_atom</th>\n      <th>g298_atom</th>\n      <th>smiles</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gdb_1</td>\n      <td>157.71180</td>\n      <td>157.709970</td>\n      <td>157.706990</td>\n      <td>0.0000</td>\n      <td>13.21</td>\n      <td>-0.3877</td>\n      <td>0.1171</td>\n      <td>0.5048</td>\n      <td>35.3641</td>\n      <td>...</td>\n      <td>-40.478930</td>\n      <td>-40.476062</td>\n      <td>-40.475117</td>\n      <td>-40.498597</td>\n      <td>6.469</td>\n      <td>-395.999595</td>\n      <td>-398.643290</td>\n      <td>-401.014647</td>\n      <td>-372.471772</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gdb_2</td>\n      <td>293.60975</td>\n      <td>293.541110</td>\n      <td>191.393970</td>\n      <td>1.6256</td>\n      <td>9.46</td>\n      <td>-0.2570</td>\n      <td>0.0829</td>\n      <td>0.3399</td>\n      <td>26.1563</td>\n      <td>...</td>\n      <td>-56.525887</td>\n      <td>-56.523026</td>\n      <td>-56.522082</td>\n      <td>-56.544961</td>\n      <td>6.316</td>\n      <td>-276.861363</td>\n      <td>-278.620271</td>\n      <td>-280.399259</td>\n      <td>-259.338802</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gdb_3</td>\n      <td>799.58812</td>\n      <td>437.903860</td>\n      <td>282.945450</td>\n      <td>1.8511</td>\n      <td>6.31</td>\n      <td>-0.2928</td>\n      <td>0.0687</td>\n      <td>0.3615</td>\n      <td>19.0002</td>\n      <td>...</td>\n      <td>-76.404702</td>\n      <td>-76.401867</td>\n      <td>-76.400922</td>\n      <td>-76.422349</td>\n      <td>6.002</td>\n      <td>-213.087624</td>\n      <td>-213.974294</td>\n      <td>-215.159658</td>\n      <td>-201.407171</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gdb_4</td>\n      <td>0.00000</td>\n      <td>35.610036</td>\n      <td>35.610036</td>\n      <td>0.0000</td>\n      <td>16.28</td>\n      <td>-0.2845</td>\n      <td>0.0506</td>\n      <td>0.3351</td>\n      <td>59.5248</td>\n      <td>...</td>\n      <td>-77.308427</td>\n      <td>-77.305527</td>\n      <td>-77.304583</td>\n      <td>-77.327429</td>\n      <td>8.574</td>\n      <td>-385.501997</td>\n      <td>-387.237686</td>\n      <td>-389.016047</td>\n      <td>-365.800724</td>\n      <td>C#C</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>gdb_5</td>\n      <td>0.00000</td>\n      <td>44.593883</td>\n      <td>44.593883</td>\n      <td>2.8937</td>\n      <td>12.99</td>\n      <td>-0.3604</td>\n      <td>0.0191</td>\n      <td>0.3796</td>\n      <td>48.7476</td>\n      <td>...</td>\n      <td>-93.411888</td>\n      <td>-93.409370</td>\n      <td>-93.408425</td>\n      <td>-93.431246</td>\n      <td>6.278</td>\n      <td>-301.820534</td>\n      <td>-302.906752</td>\n      <td>-304.091489</td>\n      <td>-288.720028</td>\n      <td>C#N</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "df = pd.read_csv(os.path.join(DATA, \"qm9.csv.gz\"), compression=\"gzip\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this talktorial we will build a model predicting one of the properties -- the dipole moment `mu` -- based on the string representation of the molecule provided in the `smiles` column in the table above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the classes and helper functions we will be using for the data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:25.679689500Z",
     "start_time": "2024-01-18T11:10:25.674683600Z"
    }
   },
   "outputs": [],
   "source": [
    "class SmilesTokenizer(object):\n",
    "    \"\"\"\n",
    "    A simple regex-based tokenizer adapted from the deepchem smiles_tokenizer package.\n",
    "    SMILES regex pattern for the tokenization is designed by Schwaller et. al., ACS Cent. Sci 5 (2019)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.regex_pattern = (\n",
    "            r\"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.\"\n",
    "            r\"|=|#|-|\\+|\\\\|\\/|:|~|@|\\?|>>?|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "        )\n",
    "        self.regex = re.compile(self.regex_pattern)\n",
    "\n",
    "    def tokenize(self, smiles):\n",
    "        \"\"\"\n",
    "        Tokenizes SMILES string.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        smiles : str\n",
    "            Input SMILES string.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[str]\n",
    "            A list of tokens.\n",
    "        \"\"\"\n",
    "        tokens = [token for token in self.regex.findall(smiles)]\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:25.884907300Z",
     "start_time": "2024-01-18T11:10:25.686904800Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocab(smiles_list, tokenizer, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of N=max_vocab_size most common tokens from list of SMILES strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles_list : List[str]\n",
    "        List of SMILES strings.\n",
    "    tokenizer : SmilesTokenizer\n",
    "    max_vocab_size : int\n",
    "        Maximum size of vocabulary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, int]\n",
    "        A dictionary that defines mapping of a token to its index in the vocabulary.\n",
    "    \"\"\"\n",
    "    tokenized_smiles = [tokenizer.tokenize(s) for s in smiles_list]\n",
    "    token_counter = Counter(c for s in tokenized_smiles for c in s)\n",
    "    tokens = [token for token, _ in token_counter.most_common(max_vocab_size)]\n",
    "    vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def smiles_to_ohe(smiles, tokenizer, vocab):\n",
    "    \"\"\"\n",
    "    Transforms SMILES string to one-hot encoding representation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str\n",
    "        Input SMILES string.\n",
    "    tokenizer : SmilesTokenizer\n",
    "    vocab : Dict[str, int]\n",
    "        A dictionary that defines mapping of a token to its index in the vocabulary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor\n",
    "        A pytorch Tensor with shape (n_tokens, vocab_size), where n_tokens is the\n",
    "        length of tokenized input string, vocab_size is the number of tokens in\n",
    "        the vocabulary\n",
    "    \"\"\"\n",
    "    unknown_token_id = len(vocab) - 1\n",
    "    token_ids = [vocab.get(token, unknown_token_id) for token in tokenizer.tokenize(smiles)]\n",
    "    ohe = torch.eye(len(vocab))[token_ids]\n",
    "    return ohe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how these functions work with a simple example. Here, we will be using the same SMILES string as in the Theoretical section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:26.037339500Z",
     "start_time": "2024-01-18T11:10:25.761560300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES string:\n",
      "\t C=CCl\n",
      "Tokens:\n",
      "\t C, =, C, Cl\n",
      "Vocab:\n",
      "\t {'C': 0, '=': 1, 'Cl': 2}\n",
      "OHE:\n",
      " [[1. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SmilesTokenizer()\n",
    "\n",
    "smiles = \"C=CCl\"\n",
    "print(\"SMILES string:\\n\\t\", smiles)\n",
    "print(\"Tokens:\\n\\t\", \", \".join(tokenizer.tokenize(smiles)))\n",
    "vocab = build_vocab([smiles], tokenizer, 3)\n",
    "print(\"Vocab:\\n\\t\", vocab)\n",
    "print(\"OHE:\\n\", np.array(smiles_to_ohe(smiles, tokenizer, vocab)).T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply these preprocessing steps to our data. For the sake of reducing the runtime, we will be using only a random subset of the whole QM9 dataset. The dataset is split into training, validation and test sets. We build the vocabulary and define the transformation of the target based only on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:31.212298100Z",
     "start_time": "2024-01-18T11:10:25.866372500Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_size = 50000\n",
    "n_train = 40000\n",
    "n_test = n_val = 5000\n",
    "\n",
    "# get a sample\n",
    "df = df.sample(n=sample_size, axis=0, random_state=42)\n",
    "\n",
    "# select columns from the data frame\n",
    "smiles = df[\"smiles\"].tolist()\n",
    "y = df[\"mu\"].to_numpy()\n",
    "\n",
    "# build a vocab using the training data\n",
    "max_vocab_size = 30\n",
    "vocab = build_vocab(smiles[:n_train], tokenizer, max_vocab_size)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# transform smiles to one-hot encoded tensors and apply padding\n",
    "X = pad_sequence(\n",
    "    sequences=[smiles_to_ohe(smi, tokenizer, vocab) for smi in smiles],\n",
    "    batch_first=True,\n",
    "    padding_value=0,\n",
    ")\n",
    "\n",
    "# normalize the target using the training data\n",
    "train_mean = y[:n_train].mean()\n",
    "train_std = y[:n_train].std()\n",
    "y = (y - train_mean) / train_std\n",
    "\n",
    "# build dataset\n",
    "data = TensorDataset(X, torch.Tensor(y))\n",
    "\n",
    "# define loaders\n",
    "ids_train = np.arange(n_train)\n",
    "ids_val = np.arange(n_val) + n_train\n",
    "ids_test = np.arange(n_test) + n_train + n_val\n",
    "train_loader = DataLoader(\n",
    "    Subset(data, ids_train),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    Subset(data, ids_val), batch_size=64, shuffle=True, generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    Subset(data, ids_test),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "In this talktorial we compare two recurrent networks. Both of them contain a single recurrent layer followed by a fully-connected layer transforming the hidden state from the last time step to a one-dimensional output. In the `RNNRegressionModel` we use a vanilla RNN architecture, while in `GRURegressionModel` a more advanced GRU architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:31.260223500Z",
     "start_time": "2024-01-18T11:10:31.224585200Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNRegressionModel(nn.Module):\n",
    "    \"\"\"Vanilla RNN with one recurrent layer\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size=32, num_layers=1):\n",
    "        \"\"\"\n",
    "        Vanilla RNN\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            The number of expected features in the input vector\n",
    "        hidden_size : int\n",
    "            The number of features in the hidden state\n",
    "\n",
    "        \"\"\"\n",
    "        super(RNNRegressionModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = out[:, -1]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GRURegressionModel(nn.Module):\n",
    "    \"\"\"GRU network with one recurrent layer\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size=32, num_layers=1):\n",
    "        \"\"\"\n",
    "        GRU network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            The number of expected features in the input vector\n",
    "        hidden_size : int\n",
    "            The number of features in the hidden state\n",
    "\n",
    "        \"\"\"\n",
    "        super(GRURegressionModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        out, hn = self.gru(x, h0)\n",
    "        out = out[:, -1]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:31.296877500Z",
     "start_time": "2024-01-18T11:10:31.241272Z"
    }
   },
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "    \"\"\"A class that provides training and validation infrastructure for the model and keeps track of training and validation metrics.\"\"\"\n",
    "\n",
    "    def __init__(self, model, lr, name=None, clip_gradients=False):\n",
    "        \"\"\"\n",
    "        Initialization.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            a model\n",
    "        lr : float\n",
    "            learning rate for one training step\n",
    "\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), self.lr)\n",
    "        self.clip_gradients = clip_gradients\n",
    "        self.model.to(device)\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.batch_loss = []\n",
    "        self.val_loss = []\n",
    "\n",
    "    def _train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_losses = []\n",
    "        for i, (X_batch, y_batch) in enumerate(loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            y_pred = self.model(X_batch)\n",
    "            loss = self.criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "\n",
    "            if self.clip_gradients:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1, norm_type=2)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        return epoch_loss / len(loader), batch_losses\n",
    "\n",
    "    def _eval_epoch(self, loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = self.model(X_batch)\n",
    "                loss = self.criterion(y_pred, y_batch.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                predictions.append(y_pred.detach().numpy())\n",
    "                targets.append(y_batch.unsqueeze(1).detach().numpy())\n",
    "\n",
    "        predictions = np.concatenate(predictions).flatten()\n",
    "        targets = np.concatenate(targets).flatten()\n",
    "        return val_loss / len(loader), predictions, targets\n",
    "\n",
    "    def train(self, train_loader, val_loader, n_epochs, print_every=10):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_loader :\n",
    "            a dataloader with training data\n",
    "        val_loader :\n",
    "            a dataloader with training data\n",
    "        n_epochs :\n",
    "            number of epochs to train for\n",
    "        \"\"\"\n",
    "        for e in range(n_epochs):\n",
    "            train_loss, train_loss_batches = self._train_epoch(train_loader)\n",
    "            val_loss, _, _ = self._eval_epoch(test_loader)\n",
    "            self.batch_loss += train_loss_batches\n",
    "            self.train_loss.append(train_loss)\n",
    "            self.val_loss.append(val_loss)\n",
    "            if e % print_every == 0:\n",
    "                print(f\"Epoch {e+0:03} | train_loss: {train_loss:.5f} | val_loss: {val_loss:.5f}\")\n",
    "\n",
    "    def validate(self, val_loader):\n",
    "        \"\"\"\n",
    "        Validate the model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        val_loader :\n",
    "            a dataloader with training data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[list, list, list]\n",
    "            Loss, y_predicted, y_target for each datapoint in val_loader.\n",
    "        \"\"\"\n",
    "        loss, y_pred, y_targ = self._eval_epoch(val_loader)\n",
    "        return loss, y_pred, y_targ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the models and train them for 50 epochs. The hidden state has the same dimension for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:31.353732Z",
     "start_time": "2024-01-18T11:10:31.293982300Z"
    }
   },
   "outputs": [],
   "source": [
    "model_rnn = ModelTrainer(\n",
    "    model=RNNRegressionModel(vocab_size, hidden_size=32),\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:46.362064600Z",
     "start_time": "2024-01-18T11:10:31.354839900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | train_loss: 1.00060 | val_loss: 0.93796\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel_rnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m51\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[22], line 79\u001B[0m, in \u001B[0;36mModelTrainer.train\u001B[1;34m(self, train_loader, val_loader, n_epochs, print_every)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;124;03mTrain the model\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;124;03m    number of epochs to train for\u001B[39;00m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_epochs):\n\u001B[1;32m---> 79\u001B[0m     train_loss, train_loss_batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m     val_loss, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_eval_epoch(test_loader)\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m train_loss_batches\n",
      "Cell \u001B[1;32mIn[22], line 36\u001B[0m, in \u001B[0;36mModelTrainer._train_epoch\u001B[1;34m(self, loader)\u001B[0m\n\u001B[0;32m     34\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(X_batch)\n\u001B[0;32m     35\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(y_pred, y_batch\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m---> 36\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclip_gradients:\n\u001B[0;32m     39\u001B[0m     torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mparameters(), max_norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, norm_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\T034\\lib\\site-packages\\torch\\_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    491\u001B[0m     )\n\u001B[1;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\T034\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model_rnn.train(train_loader, val_loader, 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:10:46.359733600Z"
    }
   },
   "outputs": [],
   "source": [
    "model_gru = ModelTrainer(\n",
    "    model=GRURegressionModel(vocab_size, hidden_size=32),\n",
    "    lr=1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:46.363731600Z",
     "start_time": "2024-01-18T11:10:46.363159100Z"
    }
   },
   "outputs": [],
   "source": [
    "model_gru.train(train_loader, val_loader, 51)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compare our models' performance during training. We have plotted the losses on the training and validation sets on each epoch. As we can see, the GRU model trains faster and has better overall performance compared to the Vanilla RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:10:46.366326400Z",
     "start_time": "2024-01-18T11:10:46.364782100Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = plt.plot(model_rnn.train_loss, label=f\"RNN train\")\n",
    "_ = plt.plot(model_rnn.val_loss, label=f\"RNN val\")\n",
    "_ = plt.plot(model_gru.train_loss, label=f\"GRU train\")\n",
    "_ = plt.plot(model_gru.val_loss, label=f\"GRU val\")\n",
    "_ = plt.xlabel(\"epoch\")\n",
    "_ = plt.ylabel(\"MSE\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further evaluate the performance of the models, we calculate the loss on the testing set and visually compare the predictions to the ground truth values. As we can see from the plot, Vanilla RNN tends to produce more skewed predictions compared to the GRU model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:10:46.366326400Z"
    }
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 2, figsize=(9, 5), constrained_layout=True)\n",
    "\n",
    "f.suptitle(\"Ground truth vs predicted values\")\n",
    "\n",
    "loss, y_pred, y_targ = model_rnn.validate(test_loader)\n",
    "print(f\"RNN test loss: {loss:.3f}\")\n",
    "\n",
    "_ = axarr[0].scatter(y_pred, y_targ, alpha=0.5, s=0.5)\n",
    "_ = axarr[0].plot(y_targ, y_targ, lw=0.4, c=\"r\")\n",
    "_ = axarr[0].set_title(\"RNN\")\n",
    "\n",
    "loss, y_pred, y_targ = model_gru.validate(test_loader)\n",
    "print(f\"GRU test loss: {loss:.3f}\")\n",
    "\n",
    "_ = axarr[1].scatter(y_pred, y_targ, alpha=0.5, s=0.5)\n",
    "_ = axarr[1].plot(y_targ, y_targ, lw=0.4, c=\"r\")\n",
    "_ = axarr[1].set_title(\"GRU\")\n",
    "\n",
    "for ax in axarr:\n",
    "    _ = ax.set_xlabel(\"y_pred\")\n",
    "    _ = ax.set_ylabel(\"y_true\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Natural language processing-based models proved to be a powerful tool for a wide range of molecular tasks. In this talktorial we introduced the basics of Recurrent Neural Network architectures and demonstrated their application to the regression task on the QM9 dataset. We have learned how to pre-process SMILES strings, build a model using PyTorch, and train the model to predict the dipole moment of the molecule. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "1. What problems do the recurrent neural networks experience during training?\n",
    "2. In this talktorial we used a regex-based tokenizer. What other tokenization approaches could be used?\n",
    "3. Why Gated recurrent unit architecture is better for capturing long-term relations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "f4fa32c2c8ac01d0df886546ca8f7fd7e0990c46c7ea867e639a9e9f26b2739e"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
