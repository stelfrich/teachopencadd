{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T037 · Uncertainty estimation\n",
    "\n",
    "**Note:** This talktorial is a part of TeachOpenCADD, a platform that aims to teach domain-specific skills and to provide pipeline templates as starting points for research projects.\n",
    "\n",
    "Authors:\n",
    "\n",
    "- Michael Backenköhler, 2022, [Volkamer lab](https://volkamerlab.org), [NextAID](https://nextaid.cs.uni-saarland.de/) project, Saarland University"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The predictive setting (and the model class) used in this talktorial is adapted from __Talktorial T022__.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim of this talktorial\n",
    "\n",
    "Researchers often focus on prediction quality alone. However, when applying a predictive model, researchers are also interested in how certain they can be in a specific prediction. Estimating and providing such information is the goal of uncertainty estimation. In this talktorial, we discuss some common methodologies and showcase ensemble methods in practice."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Theory*\n",
    "\n",
    "* Why a model can't and shouldn't be certain\n",
    "* Calibration\n",
    "* Methods overview\n",
    "    * Single deterministic methods\n",
    "    * Ensemble methods\n",
    "    * Test-time data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents in *Practical*\n",
    "* Data\n",
    "* Model\n",
    "    * Training\n",
    "    * Evaluation\n",
    "* Ensembles - Training a model multiple times\n",
    "    * Coverage of confidence intervals\n",
    "    * Calibration\n",
    "    * Ranking-based evaluation\n",
    "* Bagging ensemble - Training a model with varying data\n",
    "    * Ranking-based evaluation\n",
    "* Test-time data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### References\n",
    "* [Gawlikowski, Jakob, et al. \"A survey of uncertainty in deep neural networks.\" _arXiv preprint_ (2021), arXiv:__2107.03342__](https://arxiv.org/abs/2107.03342)\n",
    "* [Sagi, O. and Rokach, L. \"Ensemble learning: A survey\". Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(4), (2018) p.e1249.](https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1249?casa_token=1RRjvfS1_k4AAAAA%3AdR5WbRw9n8cp8wuVWx4j1ygfElNKbIJ9wXSmIeBd3C61pD1TEqX0bqswzRhNl8vY1rLDEhl29dseag)\n",
    "* [Scalia, Gabriele, et al. \"Evaluating scalable uncertainty estimation methods for deep learning-based molecular property prediction.\" _Journal of chemical information and Modeling_ __60.6__ (2020): 2697-2717](https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.9b00975)\n",
    "* __Talktorial T022__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Theory\n",
    "Often researchers pay a lot of attention to the quality of the estimation overall.\n",
    "But to apply any predictive method in practice, it is arguably as important to know how much to _trust_ an estimation.\n",
    "It would be therefore nice to have not only a point estimate of something but also some indication of how _certain_ we can be about the given estimate.\n",
    "The certainty is often modeled by replacing the point estimate with a distributional estimate.\n",
    "For example, a model $f$ on an input does not only predict $f(x)=\\hat\\theta$ but a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) $f(x)=N(\\hat\\theta, \\hat\\sigma)$.\n",
    "\n",
    "### Why a model can't and shouldn't be certain\n",
    "Before discussing how to estimate uncertainty, we take a look at the causes of uncertainty.\n",
    "Concerning the data, there are two classes of uncertainty we can distinguish.\n",
    "\n",
    "1. _Aleatoric_ or _data uncertainty_ is inherent in the data and its source\n",
    "2. _Epistemic_ is caused by the limitations of the model\n",
    "\n",
    "Aleatoric uncertainty is unavoidable because it is inherent in the real system and the way we collect data.\n",
    "It is uncertainty that remains even if one can select infinitely large sets of data.\n",
    "Imagine you are performing a chemical experiment to determine the binding affinity between some compound and a protein.\n",
    "Even if you are an outstandingly careful chemist you will probably not be able to exactly reproduce the same $K_d$ each time around.\n",
    "This uncertainty will always be present in the data. Even if you repeat the experiment until you reach old age.\n",
    "\n",
    "Epistemic uncertainty is reducible uncertainty. This is the uncertainty we can get rid of by collecting more data or improvement of the model. Oftentimes this is also referred to as model uncertainty. No machine learning model is perfect.\n",
    "We always introduce some degree of simplification and abstraction. And this introduces uncertainty into our predictions that we will keep even given perfect data. Let's consider again the binding affinity prediction from above. If we only consider molecules built using the same scaffold, the model cannot learn outside this domain. Therefore it is likely very uncertain for molecules based on other scaffolds. However, given data on other scaffolds the model can learn and consequently, the uncertainty reduces.\n",
    "\n",
    "Another aspect of uncertainty is the domain of training and test data. There is _in-domain_ uncertainty, as the uncertainty of test samples which should be \"approximately\" covered by training samples. _Out-of-domain_ and _domain-shift_ uncertainty stem from test data that is not well represented by the training data. Determining in- and out-of-domain for a given sample is a difficult problem in most domains since a good understanding of the problem is necessary to determine whether a sample is in the training domain. This problem is especially difficult in cheminformatics, where similarity metrics may fail due to _activity cliffs_. At activity cliffs, molecules may be very similar by metrics such as their fingerprint, but the target property differs drastically.\n",
    "\n",
    "### Calibration\n",
    "Assume we have a machine learning model that incorporates uncertainty. How do we evaluate and improve the predicted uncertainty?\n",
    "This is where calibration enters the picture.\n",
    "Calibration deals with the accuracy of confidence given by an uncertainty estimation. For example, a well-calibrated estimation should yield a 30%-confidence interval which should, in the limit, actually cover the true value with probability $0.3$. Oftentimes, neural network models tend to be over-confident.\n",
    "\n",
    "There are many methods to deal with the _calibration_ of an estimator. Among the most straightforward is to adjust predicted uncertainties after the training. To this end, we can use a held-out _calibration set_. This set is meant to _adjust_ the predicted uncertainties. In an over-confident model, for example, it should lead to an increase in the predicted uncertainty.\n",
    "In the practical section, we demonstrate simple scaling using a calibration set.\n",
    "\n",
    "### Methods overview\n",
    "There is a wide variety of methods providing uncertainty estimates. An excellent survey is given by [Gawlikowski et al.](https://arxiv.org/abs/2107.03342)\n",
    "Here, we stick with the most common and widely applicable methods.\n",
    "These - on the model side - can roughly be divided into\n",
    "1. single deterministic methods and\n",
    "2. ensemble methods.\n",
    "\n",
    "A model-independent method is test-time data augmentation. \n",
    "\n",
    "#### Single deterministic methods\n",
    "Arguably the most straightforward method to predict uncertainty along with the point predictor. Often this amounts to the prediction of a distribution instead of a point estimate. Consider as an example the prediction of a mean and variance parameter of a normal distribution in a regression setting. In a classification setting, we often predict class probabilities, which already is an uncertainty prediction, albeit with some constraints (see [Gawlikowski et al.](https://arxiv.org/abs/2107.03342)).\n",
    "\n",
    "Another approach is the direct prediction of uncertainty. In this case, a secondary model is trained to predict uncertainty for an already trained model. This has the obvious advantage of not needing any modification to the predictive model itself.\n",
    "\n",
    "#### Ensemble methods\n",
    "Ensemble methods build on the idea of creating a selection, i.e. an ensemble, of similar but different models. As a simple example of such an ensemble, consider a neural network model that is trained multiple times with varying random seeds. Due to the inherent randomness in the stochastic gradient descent, each trained version of the model will differ from the others.\n",
    "\n",
    "![Model ensemble](images/nn.png)\n",
    "\n",
    "*Figure 1:* \n",
    "An ensemble of similar models with different weights obtained by multiple varying training runs.\n",
    "\n",
    "This difference in the model parameters leads to a variety in the predicted values between all ensemble members. This variance can be used as an uncertainty estimate. As we will see in the practical section below, the variety in such an ensemble may often be insufficient. Often, the variety in such ensembles is too small and consequently, the uncertainty is underestimated.\n",
    "\n",
    "Additional variety can be introduced into the ensemble by varying the training data for each training run. A simple way showcased below is _bagging_ which is a shorthand for _bootstrap and aggregation_. There, for each training run, the training data is resampled with replacement leading to add variety in the resulting ensemble.\n",
    "\n",
    "We can also modify the test data for uncertainty estimation. On this side, the test data can be augmented by including slightly modified versions of the input data. Note, however, that one has to be careful with the exact methods of such modifications. Especially in chemistry, even seemingly small changes such as adding or removing a bond may have large consequences. This means, that there may be some unintentional divide between the notion of closeness in the input and the output domain of the model.\n",
    "\n",
    "Yet another way to more ensemble variety is to vary the model itself. This is done by either varying the model's architecture explicitly or via a Bayesian network with probabilistic dropout. In the latter case, we essentially have a model ensemble at test time due to the stochastic dropout.\n",
    "\n",
    "#### Test-time data augmentation\n",
    "Another way to uncertainty estimates is the use of test-time data augmentation. For each query point, we create an augmented set of points using some stochastic noise. In the practical examples of this notebook, we are dealing with fingerprint data.\n",
    "A natural way to introduce noise in such binary data is to flip bits with a small probability. This way, we obtain a set of data points for each actual query point. This, in turn, yields us a set of predictions that hopefully represents the predictive uncertainty for the query point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add short summary of what will be done in this practical section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:14:27.864588500Z",
     "start_time": "2024-01-18T11:14:19.548654800Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrdkit\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mChem\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Draw\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m     15\u001B[0m sns\u001B[38;5;241m.\u001B[39mset_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnotebook\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "\n",
    "# Silence some expected warnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:14:27.886201300Z",
     "start_time": "2024-01-18T11:14:27.884928900Z"
    }
   },
   "outputs": [],
   "source": [
    "HERE = Path(\".\").absolute()\n",
    "DATA = HERE / \"data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-use the predictive setting of __Talktorial T022__. It deals with predicting compound activity in terms of their pIC50 value to EGFR. For the prediction, we use Morgan 3 fingerprints with 2024 bits as input."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data\n",
    "We use the same data as in __Talktorial T022__. These are activities on the kinase EGFR of varying compounds found in the Chembl 25 database. The ligands are encoded using a 2048-bit Morgan 3 fingerprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.886201300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the training and test data from pickled torch tensors.\n",
    "x_train = torch.load(DATA / \"x_train\")\n",
    "y_train = torch.load(DATA / \"y_train\")\n",
    "x_test = torch.load(DATA / \"x_test\")\n",
    "y_test = torch.load(DATA / \"y_test\")\n",
    "\n",
    "# Create the data sets for training and testing.\n",
    "training_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create data loaders to iterate over training and test sets.\n",
    "training_loader = DataLoader(training_data, batch_size=64)\n",
    "test_loader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "As a model, we use a standard feed-forward network. This is similar to the one described in __Talktorial T022__. Here, however, we use _pytorch_ instead of _tensorflow_. More details specific to Pytorch can be found in the [pytorch tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.886201300Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"A simple linear forward neural network.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "We now set up the pipeline of creating and training a model.\n",
    "With the model in place, we are ready to set up the training by defining a loss function and an optimization procedure. As a loss function, we take the mean squared error since we dealing with a regression task. For the stochastic gradient descent optimization method, we choose the _Adam_ optimizer which is a standard choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T11:14:28.159769500Z",
     "start_time": "2024-01-18T11:14:27.886201300Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Run one training epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        Data loader for the training data.\n",
    "    model : torch.nn.Module\n",
    "        The model to train.\n",
    "    loss_fn : function\n",
    "        A differentiable loss function.\n",
    "    optimizer : torch.optimizer.Optimizer\n",
    "        The optimization procedure.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    \"\"\"\n",
    "    Compute the test loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        Data loader for the test data.\n",
    "    model : torch.nn.Module\n",
    "        The model.\n",
    "    loss_fn : function\n",
    "        loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    test_loss : float\n",
    "        test loss according to `loss_fn`.\n",
    "    \"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():  # faster evaluation\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    return test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To make our life simpler, when creating models, we encapsulate the model creation and training in a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.886958900Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_and_fit_model(training_loader, test_loader, epochs=8, verbose=False):\n",
    "    \"\"\"\n",
    "    Create and fit a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loader : torch.utils.data.DataLoader\n",
    "        Data loader for the training data.\n",
    "    test_loader : torch.utils.data.DataLoader\n",
    "        Data loader for the test data.\n",
    "    epochs : int, optional\n",
    "        The number of epochs to train.\n",
    "    verbose: bool, optional\n",
    "        Print the current epoch and test loss.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model: NeuralNetwork\n",
    "        A trained instance of `NeuralNetwork`.\n",
    "    \"\"\"\n",
    "    model = NeuralNetwork().to(\"cpu\")\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for i in range(epochs):\n",
    "        if verbose:\n",
    "            print(\"Epoch\", i)\n",
    "        train_loop(training_loader, model, loss_fn, optimizer)\n",
    "        test_loss = test_loop(test_loader, model, loss_fn)\n",
    "        if verbose:\n",
    "            print(f\"Test loss: {test_loss:>8f} \\n\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can now create and train a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.886958900Z"
    }
   },
   "outputs": [],
   "source": [
    "single_model = create_and_fit_model(training_loader, test_loader, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "For uncertainty estimation, we are not too concerned with prediction quality. Therefore, we just visually check the correlation between predictions and true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.886958900Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = single_model(x_test)\n",
    "sns.lmplot(\n",
    "    data=pd.DataFrame(\n",
    "        {\n",
    "            \"model\": pred.flatten().detach().numpy(),\n",
    "            \"true value\": y_test.flatten().detach().numpy(),\n",
    "        }\n",
    "    ),\n",
    "    x=\"model\",\n",
    "    y=\"true value\",\n",
    "    scatter_kws={\"s\": 3},\n",
    ")\n",
    "\n",
    "# mean absolute error\n",
    "mae_single_model = torch.abs(pred - y_test).sum().item()\n",
    "\n",
    "# mean squared error\n",
    "mse_single_model = ((pred - y_test) ** 2).sum().item()\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"The mean absolute error is {mae_single_model:.2f} and the mean squared error is {mse_single_model:.2f}.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a reasonable correlation between the predicted pIC50 and the measured value.\n",
    "It seems that our model has learned how to extract some binding affinity information from the fingerprint features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ensembles - Training a model multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.888416200Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_size = 20\n",
    "ensemble = []\n",
    "for _ in tqdm.tqdm(range(ensemble_size)):\n",
    "    training_data = TensorDataset(x_train, y_train)\n",
    "    training_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "    model = create_and_fit_model(\n",
    "        training_loader,\n",
    "        test_loader,\n",
    "    )\n",
    "    ensemble.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.891742900Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = torch.stack([model(x_test) for model in ensemble]).reshape(ensemble_size, len(x_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the predictions over all ensembles and test samples in the matrix `pred`. We can compute basic statistics for each point of test data, such as mean and variance. The variance or standard deviation is used as an uncertainty estimate of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.893747Z"
    }
   },
   "outputs": [],
   "source": [
    "stds = preds.std(0)\n",
    "var = preds.var(0)\n",
    "mean = preds.mean(0)\n",
    "mae = torch.abs(mean - y_test.flatten())\n",
    "total_mae = mae.sum().item()\n",
    "total_mse = torch.sum((mean - y_test.flatten()) ** 2).item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is interesting to compare the ensemble mean as a predictor to the single model predictor above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.898870300Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"ensemble mean\": mean.detach().numpy(),\n",
    "            \"true value\": y_test.flatten().detach().numpy(),\n",
    "        }\n",
    "    ),\n",
    "    x=\"ensemble mean\",\n",
    "    y=\"true value\",\n",
    "    scatter_kws={\"s\": 3},\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"The mean absolute error decreased from {mae_single_model:.2f} to {total_mae:.2f}. The mean squared error decreased from {mse_single_model:.2f} to {total_mse:.2f}.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the predictive quality on the test set increased.\n",
    "Originally model ensembles were introduced as a means of improving prediction quality (see [Sagi and Rokach 2018](https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1249?casa_token=1RRjvfS1_k4AAAAA%3AdR5WbRw9n8cp8wuVWx4j1ygfElNKbIJ9wXSmIeBd3C61pD1TEqX0bqswzRhNl8vY1rLDEhl29dseag)).\n",
    "Intuitively, this is achieved by mitigating the outliers of single models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverage of confidence intervals\n",
    "\n",
    "For each confidence level, we can compute confidence intervals based on the standard deviations, we get out of our model ensemble.\n",
    "According to the definition of the confidence interval for level $p$, the interval should cover the actual value with probability $p$.\n",
    "Therefore, if we plot all hit ratios over the test for all levels in $[0,1]$, we ideally would end up with a perfect diagonal (the identity function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.902476900Z"
    }
   },
   "outputs": [],
   "source": [
    "confidences = np.linspace(0, 1)\n",
    "\n",
    "hits = []\n",
    "for c in confidences:\n",
    "    delta = stds * norm.ppf(0.5 + c / 2) / np.sqrt(ensemble_size)\n",
    "    a = np.array((mean - delta < y_test.flatten()) & (mean + delta > y_test.flatten())).astype(int)\n",
    "    hits.append(a.sum() / len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.906971Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(confidences, hits)\n",
    "plt.ylabel(\"coverage\")\n",
    "plt.xlabel(\"confidence\")\n",
    "plt.yscale(\"linear\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compute the confidence curve on a small, dedicated part of the test set. Based on the confidence curve, we compute an adjustment factor for the estimated standard deviation. This process can be refined by computing a more complex transformation. For example, one could compute such a factor for any number of _bins_, i.e. intervals confidences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.912970500Z"
    }
   },
   "outputs": [],
   "source": [
    "num_calib = 100\n",
    "preds_calibration = torch.stack([model(x_test[:num_calib]) for model in ensemble]).reshape(\n",
    "    ensemble_size, num_calib\n",
    ")\n",
    "\n",
    "stds_calibration = preds_calibration.std(0)\n",
    "mean_calibration = preds_calibration.mean(0)\n",
    "confidences = np.linspace(0, 1)\n",
    "hits_calibration = []\n",
    "for c in confidences:\n",
    "    delta = stds_calibration * norm.ppf(0.5 + c / 2) / np.sqrt(ensemble_size)\n",
    "    a = np.array(\n",
    "        (mean_calibration - delta < y_test[:num_calib].flatten())\n",
    "        & (mean_calibration + delta > y_test[:num_calib].flatten())\n",
    "    ).astype(int)\n",
    "    hits_calibration.append(a.sum() / len(a))\n",
    "calibration_adjustment = (confidences / hits_calibration)[1:-1]\n",
    "calibration_adjustment = calibration_adjustment[calibration_adjustment != torch.inf].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constant calibration adjustment factor `calibration_adjustment` is used to compute confidence intervals.\n",
    "The resulting confidence curve is now much better calibrated.\n",
    "That means it is close to the identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.917495700Z"
    }
   },
   "outputs": [],
   "source": [
    "hits = []\n",
    "for c in confidences:\n",
    "    delta = (calibration_adjustment * stds * norm.ppf(0.5 + c / 2) / np.sqrt(len(ensemble)))[\n",
    "        num_calib:\n",
    "    ]\n",
    "    a = np.array(\n",
    "        (mean[num_calib:] - delta < y_test[num_calib:].flatten())\n",
    "        & (mean[num_calib:] + delta > y_test[num_calib:].flatten())\n",
    "    ).astype(int)\n",
    "    hits.append(a.sum() / len(a))\n",
    "\n",
    "plt.plot(confidences, hits)\n",
    "plt.ylabel(\"coverage\")\n",
    "plt.xlabel(\"confidence\")\n",
    "plt.yscale(\"linear\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! To get an even better calibration we could use more sophisticated calibration methods. For example, we could use an interval-based scheme. Especially if the curve is not consistent in over- or under-estimation of confidence, such a flexible scheme is more appropriate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking-based evaluation\n",
    "For now, we have not really looked at how useful our ensemble is to estimate uncertainty. Ideally, poor estimations would indicate that our prediction has -- at least potentially -- a higher error. To assess our uncertainty prediction in this sense, we can look at _confidence curves_. In this ranking-based scheme, we order our estimates by decreasing uncertainty and looking at the decrease in the total absolute error for a certain number of samples. We can then compare the resulting curve with a random baseline and the ideal curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.922494400Z"
    }
   },
   "outputs": [],
   "source": [
    "idcs_mae = torch.argsort(mae, 0, descending=True).flatten()\n",
    "idcs_conf = torch.argsort(stds, 0, descending=True).flatten()\n",
    "plt.plot(\n",
    "    (total_mae - torch.cumsum(mae[idcs_conf].flatten(), 0)).detach().numpy() / total_mae,\n",
    "    label=\"ordered by est. uncertainty\",\n",
    ")\n",
    "plt.plot(\n",
    "    [0, len(x_test)],\n",
    "    [1, 0],\n",
    "    label=\"linear decrease\",\n",
    ")\n",
    "plt.plot(\n",
    "    (total_mae - torch.cumsum(mae[idcs_mae], 0)).detach().numpy() / total_mae,\n",
    "    \"--\",\n",
    "    label=\"oracle\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"sample number\")\n",
    "plt.ylabel(\"normalized MAE\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, the blue curve, i.e. our uncertainty estimates would be somewhere in between the random baseline and the ideal one. We see some improvement over the baseline, but there is room for improvement. Often, more variety is necessary for the model ensemble.\n",
    "Below we create more variety by bootstrapping and aggregating the training data.\n",
    "Another way would be, to vary the model itself in some way within the ensemble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Bagging ensemble - Training a model with varying data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To capture the uncertainty better, we need to introduce more variety in the ensemble.\n",
    "Instead of just varying training runs, we additionally vary the training data.\n",
    "This is done by _bootstrapping and aggregating_ (_bagging_) the data:\n",
    "For each run, we resample the training data with replacement to get a (likely) slightly different training set of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.926607200Z"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_size = 20\n",
    "ensemble_bagg = []\n",
    "for _ in tqdm.tqdm(range(ensemble_size)):\n",
    "    idcs = torch.randint(low=0, high=len(x_train), size=(len(x_train),))\n",
    "    x_train_resample = x_train[idcs]\n",
    "    y_train_resample = y_train[idcs]\n",
    "    training_data = TensorDataset(x_train_resample, y_train_resample)\n",
    "    training_loader = DataLoader(training_data, batch_size=64)\n",
    "\n",
    "    model = create_and_fit_model(\n",
    "        training_loader,\n",
    "        test_loader,\n",
    "    )\n",
    "    ensemble_bagg.append(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that all ensembles are trained, we compute the test set predictions of all ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.929607900Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_bagg = torch.stack([model(x_test) for model in ensemble_bagg]).reshape(\n",
    "    ensemble_size, len(x_test)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we compute basic statistics on our ensemble predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.933612400Z"
    }
   },
   "outputs": [],
   "source": [
    "stds_bagg = preds_bagg.std(0)\n",
    "var_bagg = preds_bagg.var(0)\n",
    "mean_bagg = preds_bagg.mean(0)\n",
    "mae_bagg = torch.abs(mean_bagg - y_test.flatten())\n",
    "mse_bagg = (mean_bagg - y_test.flatten()) ** 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking-based evaluation\n",
    "We now repeat the ranking-based evaluation from above. That is, we plot the confidence curve against the constantly decreasing baseline and the optimal solution (Oracle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.935932Z"
    }
   },
   "outputs": [],
   "source": [
    "idcs_mae = torch.argsort(mae_bagg, 0, descending=True).flatten()\n",
    "idcs_conf = torch.argsort(stds_bagg, 0, descending=True).flatten()\n",
    "total_error = mae_bagg.sum().item()\n",
    "plt.plot(\n",
    "    (total_error - torch.cumsum(mae_bagg[idcs_conf].flatten(), 0)).detach().numpy() / total_error,\n",
    "    label=\"ordered by est. uncertainty bagg\",\n",
    ")\n",
    "plt.plot(\n",
    "    (total_mae - torch.cumsum(mae[idcs_conf].flatten(), 0)).detach().numpy() / total_mae,\n",
    "    label=\"ordered by est. uncertainty\",\n",
    ")\n",
    "plt.plot(\n",
    "    [0, len(x_test)],\n",
    "    [1, 0],\n",
    "    label=\"linear decrease\",\n",
    ")\n",
    "plt.plot(\n",
    "    (total_error - torch.cumsum(mae_bagg[idcs_mae], 0)).detach().numpy() / total_error,\n",
    "    \"--\",\n",
    "    label=\"oracle\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"sample number\")\n",
    "plt.ylabel(\"normalized MAE\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a consistent improvement over the baseline. This means the uncertainty\n",
    "estimates help us in getting an impression of estimate quality.\n",
    "Poorer estimates are more likely to have a higher estimated uncertainty than the better ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-time data augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we showcase test-time data augmentation. The benefit here is that it can be applied to pretty much any model.\n",
    "The basic idea is to sample _around_ each test sample.\n",
    "This set of _similar_ samples should provide information on the local variability.\n",
    "Large variability is assumed to imply higher uncertainty.\n",
    "\n",
    "In the context of fingerprints, we are dealing with bitstrings of fixed length.\n",
    "To sample around each test sample, we augment each test sample with `N_AUG` additional samples.\n",
    "In each sample and position, we introduce a mutation, i.e. a bit flip with a fixed probability (here $p=0.01$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-18T11:14:27.940300100Z"
    }
   },
   "outputs": [],
   "source": [
    "N_AUG = 100\n",
    "\n",
    "x_test_aug = torch.repeat_interleave(x_test, N_AUG, dim=0)\n",
    "\n",
    "mutations = torch.rand_like(x_test_aug) < 0.01\n",
    "x_test_aug = torch.logical_xor(mutations, x_test_aug)\n",
    "\n",
    "x_test_aug = torch.tensor(x_test_aug, dtype=torch.float32)\n",
    "\n",
    "pred = single_model(x_test_aug)\n",
    "pred = pred.reshape(-1, N_AUG, 1)\n",
    "((pred.mean(1) - y_test) ** 2).sum()\n",
    "\n",
    "stds_aug = pred.std(1)\n",
    "mae_aug = torch.abs(pred.mean(1) - y_test)\n",
    "\n",
    "torch.cumsum(mae_aug[torch.argsort(stds_aug, 0, descending=True).flatten()].flatten(), 0)\n",
    "\n",
    "idcs_mae = torch.argsort(mae_aug, 0, descending=True).flatten()\n",
    "idcs_conf = torch.argsort(stds_aug, 0, descending=True).flatten()\n",
    "total_err = mae_aug.sum().item()\n",
    "plt.plot(\n",
    "    (total_err - torch.cumsum(mae_aug[idcs_conf].flatten(), 0)).detach().numpy() / total_err,\n",
    "    label=\"ordered by est. uncertainty\",\n",
    ")\n",
    "plt.plot(\n",
    "    [0, len(x_test)],\n",
    "    [1, 0],\n",
    "    label=\"linear decrease\",\n",
    ")\n",
    "plt.plot(\n",
    "    (total_err - torch.cumsum(mae_aug[idcs_mae].flatten(), 0)).detach().numpy() / total_err,\n",
    "    \"--\",\n",
    "    label=\"oracle\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"sample number\")\n",
    "plt.ylabel(\"normalized MAE\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The field of uncertainty estimation is highly relevant to almost all machine learning applications and\n",
    "computer-aided drug discovery is no exception.\n",
    "Thus, currently, there is significant effort put into improving methodologies.\n",
    "Many issues such as explainability or objective evaluation protocols for uncertainty quantification remain to be addressed.\n",
    "However, if predictions are to be used in the real world, uncertainty cannot be ignored."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quiz\n",
    "\n",
    "1. What are the main sources of uncertainty?\n",
    "2. How could we get a model ensemble without training many models?\n",
    "3. In which scenarios can uncertainty estimation fail?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
